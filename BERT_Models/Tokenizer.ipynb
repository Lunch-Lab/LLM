{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bert Model & Bert Tokenizer \n",
    "\n",
    "* bert-base-multilinual-cases "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ijeonghun/Documents/github/Lunch_lab/LLM/Bert/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFBertTokenizer,BertTokenizer,TFBertModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래와 같이 vocab.text 를 가져와 차후 업데이트해야함\n",
    "\n",
    "다양한 음식명 등은 학습되지않았으므로"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "vacab_path=\"https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt\"\n",
    "import requests\n",
    "rq_vacab=requests.get(vacab_path)\n",
    "vocab=rq_vacab.text if rq_vacab.status_code==200 else None\n",
    "with open(\"vocab.text\",\"w\") as f:\n",
    "    f.write(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_base_tokenizer=BertTokenizer.from_pretrained(\"bert-base-multilingual-cased\",clean_up_tokenization_spaces=True)\n",
    "base_setting=bert_base_tokenizer.init_kwargs.copy()\n",
    "Own_tokenizer=BertTokenizer(vocab_file=\"vocab.text\",**base_setting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'do_lower_case': False,\n",
       " 'do_basic_tokenize': True,\n",
       " 'never_split': None,\n",
       " 'unk_token': '[UNK]',\n",
       " 'sep_token': '[SEP]',\n",
       " 'pad_token': '[PAD]',\n",
       " 'cls_token': '[CLS]',\n",
       " 'mask_token': '[MASK]',\n",
       " 'tokenize_chinese_chars': True,\n",
       " 'strip_accents': None,\n",
       " 'clean_up_tokenization_spaces': True,\n",
       " 'model_max_length': 512,\n",
       " 'tokenizer_file': '/Users/ijeonghun/.cache/huggingface/hub/models--bert-base-multilingual-cased/snapshots/3f076fdb1ab68d5b2880cb87a0886f315b8146f8/tokenizer.json',\n",
       " 'name_or_path': 'bert-base-multilingual-cased'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['안', '##녕', '##하', '##세', '##요', '.', '테', '##스트', '##용', '##입', '##니다']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Own_tokenizer.tokenize(\"안녕하세요. 테스트용입니다\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "파라미터 설명\n",
    "\n",
    "padding=True : 필요한 경우에만 패딩처리\n",
    "\n",
    "padding=max_langth : 최대 길이로 패딩처리\n",
    "\n",
    "trunaction=True : 일정 길이 이상의 문장을 잘라버림"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 9521, 118741, 35506, 24982, 48549, 119, 9866, 34994, 24974, 58303, 48345, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(Own_tokenizer.encode(\"안녕하세요. 테스트용입니다\",truncation=True,padding=\"max_length\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위에서 보다시피 BertModel은 최대 시퀀스 길이를 512로 제한하고 있습니다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorflow 모델은 아래 TFBertModel 을 사용해야합니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "model = TFBertModel.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "text = \"Replace me by any text you'd like.\"\n",
    "encoded_input = Own_tokenizer(text, return_tensors='tf')\n",
    "output = model(encoded_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "잘 작동했다면 오케이"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TFBaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=<tf.Tensor: shape=(1, 13, 768), dtype=float32, numpy=\n",
       "array([[[ 0.25244173, -0.5320835 ,  0.44963565, ...,  1.1428412 ,\n",
       "         -0.6238021 , -0.07453363],\n",
       "        [ 0.53735644,  0.07007825,  0.48796463, ...,  1.080039  ,\n",
       "         -0.5559516 , -0.55644274],\n",
       "        [ 0.46855763, -0.32863384,  0.47817138, ...,  1.1656264 ,\n",
       "         -0.738888  , -0.392017  ],\n",
       "        ...,\n",
       "        [ 0.48219374, -1.082198  ,  0.90263814, ...,  1.8029271 ,\n",
       "         -1.1342973 , -0.10343555],\n",
       "        [ 0.12828042, -0.65504766,  0.37227008, ...,  0.78174776,\n",
       "         -0.9172803 , -0.04007212],\n",
       "        [ 0.16448611, -0.52388227,  0.66378236, ...,  0.7964226 ,\n",
       "         -0.73260623, -0.26415116]]], dtype=float32)>, pooler_output=<tf.Tensor: shape=(1, 768), dtype=float32, numpy=\n",
       "array([[ 0.3479021 , -0.04663813,  0.45640564, -0.24642836, -0.05631405,\n",
       "         0.5865305 ,  0.4806916 ,  0.24746308, -0.5662966 ,  0.42599705,\n",
       "         0.01355288, -0.33559093, -0.3280591 ,  0.03906066,  0.3873652 ,\n",
       "        -0.4076553 ,  0.81105506,  0.21711051,  0.45228893, -0.5184267 ,\n",
       "        -0.99976426, -0.546474  , -0.14721583, -0.5278097 , -0.58584064,\n",
       "         0.33898953, -0.41601276,  0.39913118,  0.40122834, -0.4297543 ,\n",
       "         0.16175663, -0.9997918 ,  0.74814093,  0.79023445,  0.33251515,\n",
       "        -0.40366808,  0.13476416,  0.2373566 ,  0.2554227 , -0.16032192,\n",
       "        -0.32489136,  0.09106374, -0.4920324 ,  0.09657928, -0.08860335,\n",
       "        -0.43237305, -0.38351998,  0.39715266, -0.5554398 ,  0.21687092,\n",
       "         0.11482555,  0.325977  ,  0.49428573,  0.43520087,  0.35291722,\n",
       "         0.20356955,  0.39091527,  0.18893337,  0.5300122 , -0.46481442,\n",
       "        -0.05214422,  0.5751241 ,  0.25258154, -0.06119296, -0.22189257,\n",
       "        -0.43936864,  0.03560876, -0.33284038,  0.5684944 , -0.30362856,\n",
       "        -0.2594307 , -0.4530276 , -0.27131268,  0.10086747,  0.23006365,\n",
       "        -0.33357364,  0.551724  ,  0.40132433,  0.1830793 , -0.3137615 ,\n",
       "        -0.5992186 , -0.56757045, -0.53356713,  0.33557793, -0.29895994,\n",
       "         0.51365477,  0.2741347 , -0.5047506 ,  0.21173516, -0.0015099 ,\n",
       "         0.33089975,  0.6298417 , -0.26946092,  0.23460792, -0.3508597 ,\n",
       "        -0.4789314 , -0.89088774, -0.36981547, -0.49847007, -0.52655417,\n",
       "        -0.42902276,  0.17198889, -0.27862468, -0.36781952, -0.15284559,\n",
       "        -0.52406794,  0.32215446,  0.3790409 , -0.35111836,  0.43928993,\n",
       "         0.32998058, -0.55063653, -0.26836807,  0.22926235, -0.45898685,\n",
       "         0.9924469 , -0.5685961 ,  0.5285825 ,  0.07075632, -0.21808602,\n",
       "        -0.69418156,  0.999842  ,  0.14166108, -0.21289794,  0.2731942 ,\n",
       "         0.36536148, -0.6268287 ,  0.28037977,  0.4681144 ,  0.48059088,\n",
       "         0.46273512, -0.23916356, -0.37617832, -0.4162128 , -0.9207943 ,\n",
       "        -0.3347513 , -0.56202424,  0.27463174, -0.51683587, -0.41943827,\n",
       "         0.24014598,  0.397474  ,  0.26700902, -0.16960767, -0.32779655,\n",
       "        -0.09070767,  0.5336762 , -0.32774073,  0.99981683,  0.6998428 ,\n",
       "        -0.37948322, -0.05335687,  0.71696895, -0.7487109 , -0.3792128 ,\n",
       "        -0.3238782 , -0.33059022, -0.7023683 ,  0.34104672,  0.32977578,\n",
       "         0.30159968, -0.39092594, -0.56359833, -0.3941649 ,  0.29032296,\n",
       "        -0.74455535, -0.35516033,  0.56897956,  0.42931145,  0.4614858 ,\n",
       "        -0.29271597,  0.54565185,  0.4178389 , -0.25420663, -0.3219528 ,\n",
       "         0.53682834,  0.37994108, -0.2039564 , -0.44577777, -0.10647918,\n",
       "         0.33408922, -0.3018704 , -0.6491601 ,  0.14365971, -0.22941323,\n",
       "        -0.58674467,  0.27583587, -0.30710113, -0.07542486,  0.21406245,\n",
       "        -0.5019649 ,  0.31250784, -0.50995463,  0.43300572,  0.5638401 ,\n",
       "         0.21543029, -0.4453508 ,  0.35680056,  0.5824692 ,  0.50020033,\n",
       "         0.4782928 ,  0.1943946 ,  0.38536292,  0.33426002, -0.45308307,\n",
       "        -0.82020074,  0.57935864,  0.36210367,  0.48836362, -0.20737948,\n",
       "        -0.5436535 , -0.5643621 ,  0.7088307 ,  0.43590686, -0.32763165,\n",
       "         0.54283184,  0.4144848 , -0.4657854 , -0.36873496,  0.30617464,\n",
       "        -0.18959217, -0.55973   , -0.50040257, -0.45658213, -0.11570243,\n",
       "         0.4746388 ,  0.26908234,  0.11652774,  0.18059544, -0.27328527,\n",
       "        -0.21132085, -0.4531913 ,  0.2551606 ,  0.2581012 , -0.18873417,\n",
       "         0.9240046 , -0.341975  ,  0.17553647, -0.5977392 , -0.24857506,\n",
       "         0.36910498, -0.20933397,  0.33884004,  0.988212  ,  0.26146525,\n",
       "        -0.42480522,  0.16006275,  0.21861675,  0.27828044, -0.36229202,\n",
       "         0.19342673, -0.743106  ,  0.87178   ,  0.5191645 ,  0.3146419 ,\n",
       "        -0.9997815 ,  0.32821563,  0.16937113,  0.26928577,  0.35275602,\n",
       "         0.37180817,  0.23862508,  0.3505473 ,  0.9561053 , -0.618092  ,\n",
       "        -0.5636579 , -0.6078274 , -0.3899605 , -0.60182774, -0.35259852,\n",
       "        -0.42275956, -0.38087368, -0.3001757 , -0.20343497, -0.25917363,\n",
       "         0.37068978,  0.39688575, -0.99811304,  0.9229667 ,  0.23816869,\n",
       "        -0.23773824, -0.01355837,  0.47444877, -0.99976355,  0.4306887 ,\n",
       "        -0.24857359, -0.48189452,  0.42800462, -0.6734596 , -0.34291738,\n",
       "         0.28288352,  0.26884985,  0.45918092,  0.34480134,  0.14269881,\n",
       "         0.5621701 , -0.1547527 ,  0.07996383,  0.23959842, -0.2550753 ,\n",
       "         0.7024171 , -0.06373245,  0.24831145,  0.49015695, -0.06035559,\n",
       "         0.48240405, -0.43245968,  0.52955496,  0.54231906,  0.15485746,\n",
       "         0.23830369, -0.35337478,  0.43916473, -0.8412562 ,  0.49535483,\n",
       "        -0.02891451, -0.19621983, -0.02013824,  0.301225  , -0.2925336 ,\n",
       "        -0.22648546,  0.26618746, -0.6172161 ,  0.9997902 ,  0.40163457,\n",
       "        -0.43159142, -0.6276529 ,  0.5665579 ,  0.73981035, -0.32634267,\n",
       "        -0.88857836, -0.20339079,  0.64600474,  0.47372997,  0.23907106,\n",
       "         0.31372723,  0.23937598,  0.2662641 , -0.17215447, -0.23664197,\n",
       "        -0.07623549, -0.5350867 ,  0.47932708, -0.2669186 , -0.36551845,\n",
       "         0.4261489 , -0.41670614, -0.35334653, -0.9110322 ,  0.52367604,\n",
       "         0.47894356,  0.26681718,  0.22147746,  0.29636285, -0.3992642 ,\n",
       "         0.7121493 ,  0.6147699 , -0.34703496, -0.25792542, -0.3196536 ,\n",
       "        -0.3302127 ,  0.07031834, -0.3235714 , -0.3757293 ,  0.3013013 ,\n",
       "        -0.7673483 ,  0.2004152 , -0.17670603, -0.32362443, -0.47280803,\n",
       "         0.52177924, -0.9997873 , -0.197124  ,  0.24264848, -0.36274725,\n",
       "         0.52911526, -0.49517635, -0.15788268,  0.18506883,  0.32419884,\n",
       "         0.04531421,  0.40899214, -0.48239738,  0.32716522, -0.24022056,\n",
       "         0.3627352 ,  0.8408766 ,  0.77959716,  0.23173976, -0.41032767,\n",
       "         0.3347385 , -0.5352117 , -0.12325462,  0.50967896,  0.43533936,\n",
       "        -0.13520199,  0.36057764,  0.43458667,  0.3563604 , -0.3666633 ,\n",
       "         0.44976965, -0.26809728, -0.36706397,  0.34236088,  0.12757322,\n",
       "        -0.28108966, -0.53819907,  0.29238653, -0.57106173,  0.44750652,\n",
       "         0.33894336,  0.57663655,  0.32969406,  0.41183868, -0.3787707 ,\n",
       "        -0.32897747, -0.18657464, -0.18944047, -0.52048683, -0.5003886 ,\n",
       "        -0.35909852,  0.9998216 ,  0.41380462,  0.44355613, -0.5978199 ,\n",
       "         0.47522512,  0.29026502, -0.35272834,  0.35843188,  0.57751495,\n",
       "         0.25431   , -0.01401605,  0.28465357,  0.46114033,  0.365789  ,\n",
       "         0.5153671 ,  0.5330981 ,  0.6122617 , -0.41008103,  0.7352959 ,\n",
       "        -0.3031581 , -0.5570549 , -0.9983627 ,  0.39078313,  0.66324264,\n",
       "        -0.41900578, -0.87053585,  0.37181965, -0.35024703,  0.2785737 ,\n",
       "        -0.45568538,  0.01308941,  0.37529513, -0.30598775,  0.57645166,\n",
       "        -0.20557375,  0.9997417 , -0.23360814,  0.38720527,  0.4640775 ,\n",
       "         0.3706792 , -0.27641702, -0.47308105, -0.4289431 ,  0.40558803,\n",
       "        -0.42795447,  0.1517205 , -0.97196776,  0.45257753,  0.04240308,\n",
       "         0.3429803 , -0.22539097,  0.5788197 , -0.50425106,  0.4950579 ,\n",
       "        -0.15967911, -0.43049616, -0.38213676,  0.43496263, -0.57957816,\n",
       "         0.53889626, -0.3023781 ,  0.1203009 , -0.4168618 ,  0.31823662,\n",
       "        -0.12580834,  0.55876815, -0.23459482,  0.06801764, -0.30675802,\n",
       "        -0.3092985 , -0.4831265 ,  0.11243164, -0.33206174,  0.999792  ,\n",
       "        -0.11962219,  0.517612  , -0.40215242,  0.48178536, -0.4651917 ,\n",
       "         0.7184825 ,  0.77913535, -0.26033342,  0.4564153 ,  0.4134377 ,\n",
       "        -0.74923533,  0.49278027, -0.14870003, -0.88991994, -0.03489189,\n",
       "         0.9843483 ,  0.22870517,  0.2933127 ,  0.7056011 ,  0.44450378,\n",
       "         0.5415887 , -0.3606887 ,  0.2752793 ,  0.9174599 ,  0.2576421 ,\n",
       "         0.2042731 ,  0.33014992,  0.16904818, -0.27443036, -0.4724041 ,\n",
       "         0.99976474,  0.99972135, -0.065685  ,  0.3944671 , -0.35651562,\n",
       "        -0.3674812 , -0.38622814,  0.24905849,  0.19055149,  0.36958784,\n",
       "        -0.20294638,  0.09783694, -0.492769  , -0.22831996, -0.20883754,\n",
       "        -0.32440323, -0.36275452,  0.22377826, -0.53176856,  0.6601169 ,\n",
       "         0.46337345,  0.32482278,  0.5379677 ,  0.47536263,  0.3530689 ,\n",
       "        -0.2502074 , -0.44682613,  0.502734  , -0.42104396, -0.2843022 ,\n",
       "        -0.36260203,  0.2704361 , -0.9997232 , -0.3237366 , -0.33784482,\n",
       "        -0.36738387,  0.6854486 ,  0.39040014,  0.14912023, -0.37610722,\n",
       "        -0.23401348, -0.49936572,  0.37603018,  0.00384744,  0.38092065,\n",
       "        -0.38529932, -0.33647212,  0.542189  , -0.7119747 ,  0.27670515,\n",
       "        -0.4304193 , -0.41156307, -0.6567586 , -0.41228455, -0.3863881 ,\n",
       "         0.44902912, -0.36263102, -0.39439103,  0.51056284,  0.4320224 ,\n",
       "         0.3441305 , -0.4077765 ,  0.33110204, -0.2481204 ,  0.02812978,\n",
       "         0.5283696 ,  0.3056743 ,  0.3533516 , -0.43603942, -0.48049113,\n",
       "        -0.3101495 , -0.49682206, -0.29185447,  0.11826826, -0.37556717,\n",
       "         0.39863527, -0.3173951 ,  0.3155388 , -0.3967568 ,  0.07471466,\n",
       "         0.40073824,  0.58724654, -0.27692768,  0.64632523,  0.5545267 ,\n",
       "        -0.21092755,  0.6228992 ,  0.36475775, -0.37015784, -0.44174203,\n",
       "         0.99981725,  0.5257825 ,  0.27092704, -0.01107316, -0.19595431,\n",
       "         0.33156586,  0.35996845,  0.6898969 , -0.41675577,  0.88157123,\n",
       "        -0.41704655,  0.3141667 ,  0.53872484,  0.5202934 ,  0.04007309,\n",
       "         0.4016371 ,  0.3201568 ,  0.85655916,  0.44761527,  0.5009706 ,\n",
       "         0.54071367,  0.15733112,  0.5883252 ,  0.40442023,  0.34732214,\n",
       "         0.46462992,  0.454427  , -0.41639283,  0.45766315, -0.37495354,\n",
       "        -0.22941177, -0.07720037, -0.16801736, -0.34584978,  0.01198449,\n",
       "        -0.21146175, -0.22731487, -0.10553915,  0.504535  , -0.3114722 ,\n",
       "         0.13039503, -0.16936024, -0.3132699 ,  0.6473677 , -0.6749756 ,\n",
       "         0.45195845, -0.1843518 ,  0.12435835, -0.87156665,  0.4054451 ,\n",
       "        -0.20876074, -0.50182253, -0.30580226, -0.6826211 ,  0.37786996,\n",
       "         0.40468705, -0.4848995 ,  0.4524192 , -0.19878653,  0.31672788,\n",
       "        -0.3646297 , -0.35367754,  0.26566115, -0.9998167 ,  0.25294453,\n",
       "         0.39001054, -0.4464929 ,  0.37248138,  0.08131815,  0.313311  ,\n",
       "         0.48632511, -0.50004464, -0.57930654, -0.34302357,  0.4807937 ,\n",
       "        -0.36721832,  0.16319929,  0.4604978 , -0.5725237 , -0.28629792,\n",
       "         0.1939982 , -0.30253458,  0.38654152,  0.56753284, -0.3394163 ,\n",
       "         0.5261012 , -0.45286617,  0.15706807, -0.4972222 ,  0.26497558,\n",
       "        -0.5463125 , -0.23564523,  0.4190224 , -0.41524443, -0.639863  ,\n",
       "        -0.09325002,  0.36605805, -0.24067134,  0.2779738 ,  0.4938119 ,\n",
       "        -0.18107475,  0.43459073, -0.44992366,  0.5624225 , -0.3170777 ,\n",
       "         0.31869856, -0.89392394, -0.38683197, -0.64035356, -0.1275396 ,\n",
       "         0.3002241 ,  0.65225655,  0.16777654,  0.40102708, -0.26975816,\n",
       "         0.05095302, -0.15228233,  0.39952585,  0.41144234, -0.2814515 ,\n",
       "         0.07108262, -0.3346102 ,  0.29712665, -0.5793314 ,  0.00842018,\n",
       "        -0.997412  , -0.4989253 ,  0.26536623,  0.51733875,  0.49362367,\n",
       "        -0.36126566, -0.24618846, -0.46433792, -0.26493   ,  0.2476071 ,\n",
       "         0.38397878,  0.3993926 ,  0.42146975,  0.48070785, -0.292193  ,\n",
       "        -0.21257432,  0.8592896 , -0.3262132 ,  0.25716004,  0.45304134,\n",
       "         0.5274521 ,  0.94329584,  0.3778732 ,  0.39310294,  0.31702772,\n",
       "        -0.43416005,  0.510354  ,  0.43476295]], dtype=float32)>, past_key_values=None, hidden_states=None, attentions=None, cross_attentions=None)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comment_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import read_excel,read_csv\n",
    "from sklearn import pipeline as pip\n",
    "from sklearn import preprocessing as pre\n",
    "from sklearn import model_selection as mod\n",
    "data=read_excel(\"../Data/맞춤법검사완료.xlsx\",index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "리뷰 최대 길이를 확인해보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>place_review</th>\n",
       "      <th>감성</th>\n",
       "      <th>Spelling_checked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>천원 별로 사진에 벌집이 있어서 시켰는데 벌집없이 나옴 키오스크사진 다시보니까 사진...</td>\n",
       "      <td>부정</td>\n",
       "      <td>천원 별로 사진에 벌집이 있어서 시켰는데 벌집 없이 나옴 키오스크 사진 다시 보니까...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>제대로 안 읽은 내 잘못도 분명 있지만 사진에 저렇게 왜 해둔건지</td>\n",
       "      <td>부정</td>\n",
       "      <td>제대로 안 읽은 내 잘못도 분명 있지만 사진에 저렇게 왜 해둔 건지</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>이게 현지맛인지 모르겠지만 명 같이 갔는데 다 간이 쎄다고 했어요</td>\n",
       "      <td>부정</td>\n",
       "      <td>이게 현지 맛인지 모르겠지만 명 같이 갔는데 다 간이 세다고 했어요</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>새우는 통통하니 아주 좋았어요</td>\n",
       "      <td>긍정</td>\n",
       "      <td>새우는 통통하니 아주 좋았어요</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>정식 맛있어요</td>\n",
       "      <td>긍정</td>\n",
       "      <td>정식 맛있어요</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14368</th>\n",
       "      <td>가족들이랑 갔는데 인분이라고 하기에는 턱없이 적은 고기양이었어요</td>\n",
       "      <td>부정</td>\n",
       "      <td>가족들이랑 갔는데 인분이라고 하기에는 턱없이 적은 고기 양이었어요</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14369</th>\n",
       "      <td>야채만 가득있는 느낌이랄까 인이라고 말했는데도 그릇과 컵 전부 인만 준비해주고 더 ...</td>\n",
       "      <td>부정</td>\n",
       "      <td>야채만 가득 있는 느낌이랄까 인이라고 말했는데도 그릇과 컵 전부 인만 준비해 주고 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14370</th>\n",
       "      <td>뭐든지 맥주를 시켜도 까먹으셔서 다른분한테 또 주문 추가로 음료를 시켜도 감감무소식...</td>\n",
       "      <td>부정</td>\n",
       "      <td>뭐든지 맥주를 시켜도 까먹으셔서 다른 분한테 또 주문 추가로 음료를 시켜도 감감무소...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14371</th>\n",
       "      <td>넘 짜증나서 리뷰씁니다</td>\n",
       "      <td>부정</td>\n",
       "      <td>너무 짜증 나서 리뷰 씁니다</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14372</th>\n",
       "      <td>역시 하이디라오 존맛탱</td>\n",
       "      <td>긍정</td>\n",
       "      <td>역시 하이디라오 존맛탱</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14373 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            place_review  감성  \\\n",
       "0      천원 별로 사진에 벌집이 있어서 시켰는데 벌집없이 나옴 키오스크사진 다시보니까 사진...  부정   \n",
       "1                   제대로 안 읽은 내 잘못도 분명 있지만 사진에 저렇게 왜 해둔건지  부정   \n",
       "2                   이게 현지맛인지 모르겠지만 명 같이 갔는데 다 간이 쎄다고 했어요  부정   \n",
       "3                                       새우는 통통하니 아주 좋았어요  긍정   \n",
       "4                                                정식 맛있어요  긍정   \n",
       "...                                                  ...  ..   \n",
       "14368                가족들이랑 갔는데 인분이라고 하기에는 턱없이 적은 고기양이었어요  부정   \n",
       "14369  야채만 가득있는 느낌이랄까 인이라고 말했는데도 그릇과 컵 전부 인만 준비해주고 더 ...  부정   \n",
       "14370  뭐든지 맥주를 시켜도 까먹으셔서 다른분한테 또 주문 추가로 음료를 시켜도 감감무소식...  부정   \n",
       "14371                                       넘 짜증나서 리뷰씁니다  부정   \n",
       "14372                                       역시 하이디라오 존맛탱  긍정   \n",
       "\n",
       "                                        Spelling_checked  \n",
       "0      천원 별로 사진에 벌집이 있어서 시켰는데 벌집 없이 나옴 키오스크 사진 다시 보니까...  \n",
       "1                  제대로 안 읽은 내 잘못도 분명 있지만 사진에 저렇게 왜 해둔 건지  \n",
       "2                  이게 현지 맛인지 모르겠지만 명 같이 갔는데 다 간이 세다고 했어요  \n",
       "3                                       새우는 통통하니 아주 좋았어요  \n",
       "4                                                정식 맛있어요  \n",
       "...                                                  ...  \n",
       "14368               가족들이랑 갔는데 인분이라고 하기에는 턱없이 적은 고기 양이었어요  \n",
       "14369  야채만 가득 있는 느낌이랄까 인이라고 말했는데도 그릇과 컵 전부 인만 준비해 주고 ...  \n",
       "14370  뭐든지 맥주를 시켜도 까먹으셔서 다른 분한테 또 주문 추가로 음료를 시켜도 감감무소...  \n",
       "14371                                    너무 짜증 나서 리뷰 씁니다  \n",
       "14372                                       역시 하이디라오 존맛탱  \n",
       "\n",
       "[14373 rows x 3 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "348\n",
      "357\n"
     ]
    }
   ],
   "source": [
    "print(data.place_review.map(lambda x:len(x)).max())\n",
    "print(data.Spelling_checked.map(lambda x:len(x)).max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위와 같이 길이는 문제가 없습니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['국', '##수']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Own_tokenizer.tokenize(\"국수\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위와 같이 음식 고유명사에 대한 vocab.txt 갱신이 필요할 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"감성\"]=data.감성.map(lambda x:1 if x==\"긍정\" else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5차 5겹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "감성\n",
       "1    12509\n",
       "0     1864\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.감성.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위와 같이 데이터 불균형이 굉장히 심함.\n",
    "\n",
    "따라서 다음과 같이 처리하겠습니다.\n",
    "\n",
    "1. 5X5 Kfold with stratified\n",
    "\n",
    "2. undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from tqdm import tqdm\n",
    "kfolds=mod.RepeatedStratifiedKFold(n_repeats=5,n_splits=5,random_state=1)\n",
    "undersampling=RandomUnderSampler(random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dc={}\n",
    "for i,(train,test) in enumerate(kfolds.split(X=data.Spelling_checked,y=data.감성)):\n",
    "    sub_train=data.loc[train]\n",
    "    sub_test=data.loc[test]\n",
    "\n",
    "    # 훈련시 불균형 해소를 위해\n",
    "    sampling_train=undersampling.fit_resample(X=sub_train[\"Spelling_checked\"].to_frame(),y=sub_train[\"감성\"].to_frame())\n",
    "    data_dc[i]=(sampling_train,sub_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위를 기본으로 BertModel에 맞게 바꾸어야함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import DataFrame,merge\n",
    "def trans_data_for_bert(X:DataFrame,tokenizer=Own_tokenizer):\n",
    "      feature_col=\"Spelling_checked\"\n",
    "      label_col=\"감성\"\n",
    "\n",
    "      trans_fuction=lambda sentence:tokenizer.encode_plus(sentence,truncation=True,padding=\"max_length\")\n",
    "      #trans=X[feature_col].map(trans_fuction)\n",
    "      trans=[]\n",
    "      for i in tqdm(X.index):\n",
    "            trans.append(trans_fuction(X.loc[i,feature_col]))\n",
    "\n",
    "      temp_={key:[value[key] for value in trans] for key in ['input_ids', 'token_type_ids', 'attention_mask']}\n",
    "      return temp_,X[label_col].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 2119.41it/s]\n"
     ]
    }
   ],
   "source": [
    "test_trans=trans_data_for_bert(data.loc[:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[101, 9746, 14279, 9353, 11261, 9405, 18623, 10530, 9339, 38696, 10739, 90587, 9485, 119305, 41850, 9339, 38696, 73610, 8982, 119156, 9838, 28188, 12605, 20308, 9405, 18623, 25805, 9356, 25503, 118671, 9405, 18623, 86933, 9339, 38696, 9960, 16323, 11664, 9429, 16758, 15303, 9339, 38696, 8964, 9448, 12605, 96279, 48549, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 9672, 37601, 9521, 9642, 10892, 8996, 9654, 118940, 12092, 9367, 16758, 76123, 9405, 18623, 10530, 9663, 82838, 9596, 9960, 118804, 8865, 12508, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 9638, 14153, 9978, 12508, 9254, 12030, 12508, 9283, 31401, 118632, 28578, 9281, 38401, 8852, 41850, 9056, 8845, 10739, 9435, 85634, 9965, 12965, 48549, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 9415, 27355, 11018, 9879, 43022, 35506, 25503, 9519, 16323, 9685, 119118, 12965, 48549, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 9670, 21155, 9254, 119192, 12965, 48549, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]}\n"
     ]
    }
   ],
   "source": [
    "print(test_trans[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2982/2982 [00:00<00:00, 6017.34it/s]\n",
      "100%|██████████| 2875/2875 [00:00<00:00, 6302.98it/s]\n",
      "100%|██████████| 2982/2982 [00:00<00:00, 6036.00it/s]\n",
      "100%|██████████| 2875/2875 [00:00<00:00, 5978.72it/s]\n",
      "100%|██████████| 2982/2982 [00:00<00:00, 6129.86it/s]\n",
      "100%|██████████| 2875/2875 [00:00<00:00, 6614.16it/s]\n",
      "100%|██████████| 2982/2982 [00:00<00:00, 3714.14it/s]\n",
      "100%|██████████| 2874/2874 [00:00<00:00, 6328.75it/s]\n",
      "100%|██████████| 2984/2984 [00:00<00:00, 6072.61it/s]\n",
      "100%|██████████| 2874/2874 [00:00<00:00, 6430.26it/s]\n",
      "100%|██████████| 2982/2982 [00:00<00:00, 6160.91it/s]\n",
      "100%|██████████| 2875/2875 [00:00<00:00, 6374.55it/s]\n",
      "100%|██████████| 2982/2982 [00:00<00:00, 5985.16it/s]\n",
      "100%|██████████| 2875/2875 [00:00<00:00, 6469.69it/s]\n",
      "100%|██████████| 2982/2982 [00:00<00:00, 6096.46it/s]\n",
      "100%|██████████| 2875/2875 [00:00<00:00, 3006.28it/s]\n",
      "100%|██████████| 2982/2982 [00:00<00:00, 6042.32it/s]\n",
      "100%|██████████| 2874/2874 [00:00<00:00, 6064.60it/s]\n",
      "100%|██████████| 2984/2984 [00:00<00:00, 6137.86it/s]\n",
      "100%|██████████| 2874/2874 [00:00<00:00, 6337.07it/s]\n",
      "100%|██████████| 2982/2982 [00:00<00:00, 6111.46it/s]\n",
      "100%|██████████| 2875/2875 [00:00<00:00, 6551.44it/s]\n",
      "100%|██████████| 2982/2982 [00:00<00:00, 6085.80it/s]\n",
      "100%|██████████| 2875/2875 [00:00<00:00, 6336.55it/s]\n",
      "100%|██████████| 2982/2982 [00:00<00:00, 6085.54it/s]\n",
      "100%|██████████| 2875/2875 [00:01<00:00, 2396.05it/s]\n",
      "100%|██████████| 2982/2982 [00:00<00:00, 5978.34it/s]\n",
      "100%|██████████| 2874/2874 [00:00<00:00, 6416.72it/s]\n",
      "100%|██████████| 2984/2984 [00:00<00:00, 6077.10it/s]\n",
      "100%|██████████| 2874/2874 [00:00<00:00, 6314.44it/s]\n",
      "100%|██████████| 2982/2982 [00:00<00:00, 5874.89it/s]\n",
      "100%|██████████| 2875/2875 [00:00<00:00, 6365.58it/s]\n",
      "100%|██████████| 2982/2982 [00:00<00:00, 6115.11it/s]\n",
      "100%|██████████| 2875/2875 [00:00<00:00, 6479.88it/s]\n",
      "100%|██████████| 2982/2982 [00:00<00:00, 6125.72it/s]\n",
      "100%|██████████| 2875/2875 [00:00<00:00, 6387.13it/s]\n",
      "100%|██████████| 2982/2982 [00:01<00:00, 1963.99it/s]\n",
      "100%|██████████| 2874/2874 [00:00<00:00, 6374.17it/s]\n",
      "100%|██████████| 2984/2984 [00:00<00:00, 5987.01it/s]\n",
      "100%|██████████| 2874/2874 [00:00<00:00, 6414.82it/s]\n",
      "100%|██████████| 2982/2982 [00:00<00:00, 6075.54it/s]\n",
      "100%|██████████| 2875/2875 [00:00<00:00, 6511.46it/s]\n",
      "100%|██████████| 2982/2982 [00:00<00:00, 6027.18it/s]\n",
      "100%|██████████| 2875/2875 [00:00<00:00, 6427.04it/s]\n",
      "100%|██████████| 2982/2982 [00:00<00:00, 5730.39it/s]\n",
      "100%|██████████| 2875/2875 [00:00<00:00, 6461.07it/s]\n",
      "100%|██████████| 2982/2982 [00:00<00:00, 6088.40it/s]\n",
      "100%|██████████| 2874/2874 [00:00<00:00, 6413.27it/s]\n",
      "100%|██████████| 2984/2984 [00:00<00:00, 6090.95it/s]\n",
      "100%|██████████| 2874/2874 [00:00<00:00, 6448.25it/s]\n"
     ]
    }
   ],
   "source": [
    "kfold_list={\"train\":[],\"test\":[]}\n",
    "for i,(train,test) in enumerate(kfolds.split(X=data.Spelling_checked,y=data.감성)):\n",
    "    sub_train=data.loc[train]\n",
    "    sub_test=data.loc[test]\n",
    "\n",
    "    #훈련시 불균형 해소를 위해\n",
    "    sampling_train=undersampling.fit_resample(X=sub_train[\"Spelling_checked\"].to_frame(),y=sub_train[\"감성\"].to_frame())\n",
    "    kfold_list[\"train\"].append(trans_data_for_bert(merge(*sampling_train,left_index=True,right_index=True)))\n",
    "\n",
    "    kfold_list[\"test\"].append(trans_data_for_bert(sub_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(kfold_list[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])\n",
      "512\n"
     ]
    }
   ],
   "source": [
    "print(Own_tokenizer.encode_plus(\"테스트용 문장입니다\",truncation=True,padding=\"max_length\").keys())\n",
    "print(len(Own_tokenizer.encode_plus(\"테스트용 문장입니다\",truncation=True,padding=\"max_length\")[\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow import int32\n",
    "\n",
    "input_ids=keras.layers.Input(shape=(512,),dtype=int32,name=\"input_ids\")\n",
    "attention_mask=keras.layers.Input(shape=(512,),dtype=int32,name=\"attention_mask\")\n",
    "token_type_id=keras.layers.Input(shape=(512,),dtype=int32,name=\"token_type_id\")\n",
    "bert_output=model([input_ids,attention_mask,token_type_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KerasTensor: shape=(None, 768) dtype=float32 (created by layer 'tf_bert_model')>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#우리는 아래의 pooler_output 을 사용해야합니다\n",
    "bert_output.pooler_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_layer=keras.layers.Dense(1, activation='sigmoid', kernel_initializer=keras.initializers.TruncatedNormal(stddev=0.02))(bert_output.pooler_output)\n",
    "sentiment_model=keras.Model([input_ids,attention_mask,token_type_id],sentiment_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)      [(None, 512)]                0         []                            \n",
      "                                                                                                  \n",
      " attention_mask (InputLayer  [(None, 512)]                0         []                            \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " token_type_id (InputLayer)  [(None, 512)]                0         []                            \n",
      "                                                                                                  \n",
      " tf_bert_model (TFBertModel  TFBaseModelOutputWithPooli   1778534   ['input_ids[0][0]',           \n",
      " )                           ngAndCrossAttentions(last_   40         'attention_mask[0][0]',      \n",
      "                             hidden_state=(None, 512, 7              'token_type_id[0][0]']       \n",
      "                             68),                                                                 \n",
      "                              pooler_output=(None, 768)                                           \n",
      "                             , past_key_values=None, hi                                           \n",
      "                             dden_states=None, attentio                                           \n",
      "                             ns=None, cross_attentions=                                           \n",
      "                             None)                                                                \n",
      "                                                                                                  \n",
      " dense (Dense)               (None, 1)                    769       ['tf_bert_model[0][1]']       \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 177854209 (678.46 MB)\n",
      "Trainable params: 177854209 (678.46 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "sentiment_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "잘 만들어졌네요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TPU = True\n",
    "# if TPU:\n",
    "#   resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='grpc://' + os.environ['COLAB_TPU_ADDR'])\n",
    "#   tf.config.experimental_connect_to_cluster(resolver)\n",
    "#   tf.tpu.experimental.initialize_tpu_system(resolver)\n",
    "# else:\n",
    "#   pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "transformer model 에 추천되는 optimizer 설치\n",
    "\n",
    "버전 에러가 일어남"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tensorflow_addons\n",
    "#learning_rate=1e-6,weight_decay=0.0025, warmup_proportion=0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer=keras.optimizers.AdamW(learning_rate=1e-6,weight_decay=0.0025) #로컬환경에서 느림"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping=keras.callbacks.EarlyStopping(monitor=\"val_accuracy\",patience=5,restore_best_weights=True)\n",
    "check_point=keras.callbacks.ModelCheckpoint(filepath=\".\",save_best_only=True,monitor=\"val_accuracy\",mode=\"min\")\n",
    "\n",
    "callback=[early_stopping,check_point]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "Optimizer=keras.optimizers.legacy.Adam(learning_rate=1e-6)\n",
    "loss=keras.losses.BinaryCrossentropy()\n",
    "\n",
    "sentiment_model.compile(optimizer=Optimizer,loss=loss,metrics=[\"accuracy\",\"f1_score\",\"recall\",\"precision\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['train', 'test'])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kfold_list.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history=sentiment_model.fit(kfold_list[\"train\"],epochs=100,shuffle=True,validation_data=kfold_list[\"test\"],callbacks=callback,batch_size=100,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bert",
   "language": "python",
   "name": "bert"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
